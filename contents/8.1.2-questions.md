#### 8.1.2 Questions

1. [E] What are the basic assumptions to be made for linear regression?
2. [E] What happens if we don’t apply feature scaling to logistic regression?
3. [E] What are the algorithms you’d use when developing the prototype of a fraud detection model?
4. Feature selection.
    1. [E] Why do we use feature selection?
    2. [M] What are some of the algorithms for feature selection? Pros and cons of each.
5. k-means clustering.
    1. [E] How would you choose the value of k?
    1. [E] If the labels are known, how would you evaluate the performance of your k-means clustering algorithm?
    1. [M] How would you do it if the labels aren’t known?
    1. [H] Given the following dataset, can you predict how K-means clustering works on it? Explain.

    <center>
      <img src="images/image28.png" width="45%" alt="k-means clustering" title="image_tooltip">
    </center>
6. k-nearest neighbor classification.
    1. [E] How would you choose the value of k?
    1. [E] What happens when you increase or decrease the value of k?
    1. [M] How does the value of k impact the bias and variance?
7. k-means and GMM are both powerful clustering algorithms.
    1. [M] Compare the two.
    1. [M] When would you choose one over another?
    
    **Hint**: Here’s an example of how K-means and GMM algorithms perform on the artificial mouse dataset.

    <center>
      <img src="images/image29.png" width="95%" alt="k-means clustering vs. gaussian mixture model" title="image_tooltip"><br>
      Image from <a href="https://www.mghassany.com/MLcourse/gaussian-mixture-models-em.html">Mohamad Ghassany’s course on Machine Learning</a>
    </center>
8. Bagging and boosting are two popular ensembling methods. Random forest is a bagging example while XGBoost is a boosting example.
    1. [M] What are some of the fundamental differences between bagging and boosting algorithms?
    1. [M] How are they used in deep learning?
9. Given this directed graph.
    <center>
      <img src="images/image30.png" width="30%" alt="Adjacency matrix" title="image_tooltip"><br>
    </center>
    1. [E] Construct its adjacency matrix.
    1. [E] How would this matrix change if the graph is now undirected?
    1. [M] What can you say about the adjacency matrices of two isomorphic graphs?
10. Imagine we build a user-item collaborative filtering system to recommend to each user items similar to the items they’ve bought before.
    1. [M] You can build either a user-item matrix or an item-item matrix. What are the pros and cons of each approach?
    1. [E] How would you handle a new user who hasn’t made any purchases in the past?
11. [E] Is feature scaling necessary for kernel methods?
12. Naive Bayes classifier.
    19. [E] How is Naive Bayes classifier naive?
    20. [M] Let’s try to construct a Naive Bayes classifier to classify whether a tweet has a positive or negative sentiment. We have four training samples:

      <table>
        <tr>
         <td>
      <strong>Tweet</strong>
         </td>
         <td><strong>Label</strong>
         </td>
        </tr>
        <tr>
         <td>This makes me so upset
         </td>
         <td>Negative
         </td>
        </tr>
        <tr>
         <td>This puppy makes me happy
         </td>
         <td>Positive
         </td>
        </tr>
        <tr>
         <td>Look at this happy hamster
         </td>
         <td>Positive
         </td>
        </tr>
        <tr>
         <td>No hamsters allowed in my house
         </td>
         <td>Negative
         </td>
        </tr>
      </table>

    According to your classifier, what's sentiment of the sentence `The hamster is upset with the puppy`?

13. Two popular algorithms for winning Kaggle solutions are Light GBM and XGBoost. They are both gradient boosting algorithms.
    1. [E] What is gradient boosting?
    1. [M] What problems is gradient boosting good for?
14. SVM.
    1. [E] What’s linear separation? Why is it desirable when we use SVM?
    1. [M] How well would vanilla SVM work on this dataset?

      <center>
        <img src="images/image31.png" width="30%" alt="Adjacency matrix" title="image_tooltip"><br>
      </center>

    1. [M] How well would vanilla SVM work on this dataset?

      <center>
        <img src="images/image32.png" width="30%" alt="Adjacency matrix" title="image_tooltip"><br>
      </center>

    1. [M] How well would vanilla SVM work on this dataset?
      <center>
        <img src="images/image33.png" width="27%" alt="Adjacency matrix" title="image_tooltip"><br>
      </center>

*This book was created by [Chip Huyen](https://huyenchip.com) with the help of wonderful friends. For feedback, errata, and suggestions, the author can be reached [here](https://huyenchip.com/communication/). Copyright ©2021 Chip Huyen.*