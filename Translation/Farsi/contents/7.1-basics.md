### 7.1 مبانی

1. [E] یادگیری تحت نظارت، بدون نظارت، ضعیف نظارت، نیمه نظارت و فعال را توضیح دهید.
2. به حداقل رساندن ریسک تجربی.
     1. [E] ریسک در به حداقل رساندن تجربی ریسک چیست؟
     2. [E] چرا تجربی است؟
     3. [E] چگونه این خطر را به حداقل برسانیم؟
3. [E] تیغ Occam بیان می کند که وقتی توضیح ساده و توضیح پیچیده هر دو به یک اندازه خوب کار می کنند، توضیح ساده معمولاً صحیح است. چگونه این اصل را در ML اعمال کنیم؟
4. [E] چه شرایطی باعث شد تا یادگیری عمیق در دهه گذشته محبوبیت پیدا کند؟
5. [M] اگر یک NN گسترده و یک NN عمیق با تعداد پارامترهای یکسان داشته باشیم، کدام یک گویاتر است و چرا؟
6. [H] قضیه تقریب جهانی بیان می‌کند که یک شبکه عصبی با 1 لایه پنهان می‌تواند هر تابع پیوسته را برای ورودی‌های یک محدوده خاص تقریبی کند. پس چرا یک شبکه عصبی ساده نمی تواند به یک خطای مثبت دلخواه کوچک برسد؟
7. [E] نقاط زین و مینیمم های محلی چیست؟ تصور می شود کدامیک برای آموزش NN های بزرگ مشکلات بیشتری ایجاد می کنند؟
8. فراپارامترها.
     4. [E] تفاوت بین پارامترها و فراپارامترها چیست؟
     5. [E] چرا تنظیم فراپارامتر مهم است؟
     6. [M] الگوریتم تنظیم فراپارامترها را توضیح دهید.
9. طبقه بندی در مقابل رگرسیون.
     7. [E] چه چیزی یک مسئله طبقه بندی را با یک مشکل رگرسیونی متفاوت می کند؟
     8. [E] آیا می توان یک مسئله طبقه بندی را به یک مسئله رگرسیونی تبدیل کرد و بالعکس؟
10. روش های پارامتریک در مقابل ناپارامتریک.
     9. [E] تفاوت بین روش های پارامتری و روش های ناپارامتریک چیست؟ از هر روش یک مثال بزنید.
     10. [H] چه زمانی باید از یکی و چه زمانی از دیگری استفاده کنیم؟
11. [M] چرا مجموعه مدل‌های آموزش‌دیده مستقل عموماً عملکرد را بهبود می‌بخشد؟
12. [M] چرا منظم‌سازی L1 به پراکندگی منجر می‌شود در حالی که منظم‌سازی L2 وزن‌ها را به 0 نزدیک‌تر می‌کند؟
13. [E] چرا عملکرد یک مدل ML در تولید کاهش می یابد؟
14. [M] هنگام استقرار مدل های بزرگ یادگیری ماشینی ممکن است با چه مشکلاتی مواجه شویم؟
15. مدل شما در مجموعه تست عملکرد بسیار خوبی دارد اما در تولید ضعیف است.
     11. [م] فرضیه های شما در مورد علل چیست؟
     12. [H] چگونه صحت فرضیه های خود را تأیید می کنید؟
     13. [M] تصور کنید فرضیه های شما در مورد علل درست است. برای رسیدگی به آنها چه می کنید؟