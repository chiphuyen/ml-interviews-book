### 8.3 آموزش شبکه های عصبی

> 🌳 **نکته** 🌳<br>
برای نکات بیشتر در مورد آموزش شبکه های عصبی، بررسی کنید:
- [یک دستور العمل برای آموزش شبکه های عصبی] (http://karpathy.github.io/2019/04/25/recipe/) (Karpathy 2019)
- [NLP's Clever Hans Moment has Arrived] (https://thegradient.pub/nlps-clever-hans-moment-has-arrived/) (Heinzerling 2019): نوشته ای عالی در تلاش برای درک آنچه شبکه عصبی شما دقیقاً می آموزد، و تکنیک هایی برای اطمینان از اینکه مدل شما به درستی با داده های متنی کار می کند.
- [مروری بر الگوریتم‌های بهینه‌سازی گرادیان نزول](http://ruder.io/optimizing-gradient-descent/index.html) (Ruder 2016)

41. [E] هنگام ساختن یک شبکه عصبی، آیا باید ابتدا آن را بیش از حد نصب کرد یا کمتر؟
42. [E] بروزرسانی گرادیان وانیلی را بنویسید.
43. شبکه عصبی در Numpy ساده.
     26. [E] گذر رو به جلو و عقب را برای یک شبکه عصبی پیشروی دولایه با یک لایه ReLU در بین آن به صورت NumPy بنویسید.
     27. [M] برای پاس رو به جلو و عقب در NumPy حذف وانیلی را اجرا کنید.
44. توابع فعال سازی.
     28. [E] نمودارهای سیگموئید، tanh، ReLU، و ReLU نشتی را رسم کنید.
     29. [E] جوانب مثبت و منفی هر تابع فعال سازی.
     30. [E] آیا ReLU قابل تمایز است؟ وقتی قابل تمایز نیست چه باید کرد؟
     31. [M] مشتقات تابع سیگموید $$\sigma(x)$$ را زمانی که $$x$$ یک بردار است استخراج کنید.
45. [E] انگیزه رد شدن از اتصال در کارهای عصبی چیست؟
46. ناپدید شدن و انفجار شیب.
     32. [E] چگونه می دانیم که گرادیان ها در حال انفجار هستند؟ چگونه از آن جلوگیری کنیم؟
     33. [E] چرا RNN ها به ویژه در برابر ناپدید شدن و انفجار شیب حساس هستند؟
47. [M] نرمال سازی وزن، هنجار بردار وزن را از گرادیان آن جدا می کند. چگونه به آموزش کمک می کند؟
48. [M] هنگام آموزش یک شبکه عصبی بزرگ، مثلاً یک مدل زبان با یک میلیارد پارامتر، مدل خود را بر اساس مجموعه اعتبارسنجی در پایان هر دوره ارزیابی می کنید. متوجه می شوید که تلفات اعتبارسنجی شما اغلب کمتر از ضرر قطار شما است. چه اتفاقی ممکن است رخ دهد؟
49. [E] از چه معیارهایی برای توقف زودهنگام استفاده می کنید؟
50. [E] نزول گرادیان در مقابل SGD در مقابل مینی دسته ای SGD.
51. [H] آموزش مدل‌های یادگیری عمیق با استفاده از دوره‌ها یک روش معمول است: ما از داده‌ها نمونه‌برداری می‌کنیم **بدون** جایگزین. چرا به جای نمونه برداری از داده ها **با ** جایگزین از دوره ها استفاده کنیم؟
52. [M] وزن مدل شما در طول تمرین بسیار نوسان می کند. چگونه بر عملکرد مدل شما تأثیر می گذارد؟ در مورد آن چه باید کرد؟
53. میزان یادگیری.
     34. [E] نمودار تعداد دوره های آموزشی در مقابل خطای آموزش را برای زمانی که نرخ یادگیری است رسم کنید:
         1. خیلی بالا
         2. خیلی کم
         3. قابل قبول
     35. [E] گرم کردن میزان یادگیری چیست؟ چرا ما به اون احتیاج داریم؟
54. [E] هنجار دسته ای و هنجار لایه را مقایسه کنید.
55. [M] چرا هنجار مربع L2 گاهی اوقات برای منظم کردن شبکه های عصبی به هنجار L2 ترجیح داده می شود؟
56. [E] برخی از مدل ها از کاهش وزن استفاده می کنند: پس از هر به روز رسانی گرادیان، وزن ها در ضریب کمی کمتر از 1 ضرب می شوند. این برای چه چیزی مفید است؟
57. کاهش نرخ یادگیری در طول آموزش یک روش معمول است.
     36. [E] انگیزه چیست؟
     37. [M] چه مواردی ممکن است استثنا باشند؟
58. اندازه دسته.
     38. [E] وقتی اندازه دسته را به 1 کاهش می دهید، چه اتفاقی برای آموزش مدل شما می افتد؟
     39. [E] وقتی از کل داده های آموزشی به صورت دسته ای استفاده می کنید چه اتفاقی می افتد؟
     40. [M] چگونه باید نرخ یادگیری را با افزایش یا کاهش اندازه دسته تنظیم کنیم؟
59. [M] چرا Adagrad گاهی اوقات در مشکلات با گرادیان های پراکنده مورد علاقه است؟
60. آدم در مقابل SGD.
     41. [M] در مورد توانایی همگرایی و تعمیم Adam در مقابل SGD چه می توانید بگویید؟
     42. [M] چه چیز دیگری می توانید در مورد تفاوت بین این دو بهینه ساز بگویید؟
61. [M] با موازی سازی مدل، می توانید وزن مدل خود را با استفاده از گرادیان های هر ماشین به صورت ناهمزمان یا همزمان به روز کنید. مزایا و معایب SGD ناهمزمان در مقابل SGD سنکرون چیست؟
62. [M] چرا نباید دو لایه خطی متوالی در یک شبکه عصبی داشته باشیم؟
63. [M] آیا یک شبکه عصبی تنها با RELU (غیر خطی) می تواند به عنوان یک طبقه بندی خطی عمل کند؟
64. [M] کوچکترین شبکه عصبی را طراحی کنید که می تواند به عنوان یک دروازه XOR عمل کند.
65. [E] چرا ما فقط تمام وزن ها را در یک شبکه عصبی به صفر مقداردهی نمی کنیم؟
66. تصادفی.
     43. [M] برخی از منابع تصادفی در یک شبکه عصبی چیست؟
     44. [M] گاهی اوقات تصادفی بودن هنگام آموزش شبکه های عصبی مطلوب است. چرا اینطور است؟
67. نورون مرده.
     45. [E] نورون مرده چیست؟
     46. [E] چگونه آنها را در شبکه عصبی خود تشخیص دهیم؟
     47. [M] چگونه از آنها جلوگیری کنیم؟
68. هرس.
     48. [M] هرس یک تکنیک محبوب است که در آن وزن های خاصی از یک شبکه عصبی روی 0 تنظیم می شود. چرا مطلوب است؟
     49. [M] چگونه انتخاب می کنید که چه چیزی را از شبکه عصبی هرس کنید؟
69. [H] تحت چه شرایطی امکان بازیابی اطلاعات تمرینی از پست های بازرسی وزن وجود دارد؟
70. [H] چرا سعی می کنیم به جای آموزش یک مدل کوچک از ابتدا، اندازه یک مدل بزرگ آموزش دیده را از طریق تکنیک هایی مانند تقطیر دانش کاهش دهیم؟

---
*این کتاب